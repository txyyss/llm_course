{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec3. LangChain\n",
    "\n",
    "\n",
    "**LangChain** is a framework for developing applications powered by language models. It provides abundant abstractions about langage models and sources of context (prompt instructions, few shot examples, content to ground its response in, etc.), which enable the user to easily **chain** these components together for developing awesome applications.\n",
    "\n",
    "In this lab, we will learn several key abstractions in LangChain and build an input-output customized AI-powered web-search application.\n",
    "\n",
    "### Reference \n",
    "1. [Langchain document](https://python.langchain.com/docs/introduction/)\n",
    "\n",
    "\n",
    "## 0. First thing first\n",
    "\n",
    "### 0.1 Dependencies and Keys\n",
    "  \n",
    "In addition to the Open AI keys, add the following keys to your .env file.\n",
    "\n",
    "- Serp api key:\n",
    "    ```\n",
    "    SERPAPI_API_KEY=\"YOURKEY\"\n",
    "    ```\n",
    "    The `SERPAPI_API_KEY` is for invoking the search engine, first register through this [web site](https://serpapi.com/).\n",
    "\n",
    "    After getting these two keys, set your keys as environment variables.\n",
    "- Langchain API key (for tracing)\n",
    "    ```\n",
    "    LANGCHAIN_TRACING_V2=\"true\"\n",
    "    LANGCHAIN_API_KEY=ls_xxxxxxxx\n",
    "    ```\n",
    "    To create a `LANGCHAIN_API_KEY`, you can register through the [LANGSMITH](https://docs.smith.langchain.com/)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have installed these dependencies in your image\n",
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv  \n",
    "import os  \n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL=\"qwen2.5-72b-instruct\"\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.environ.get(\"INFINI_API_KEY\")  # langchain use this environment variable to find the OpenAI API key\n",
    "OPENAI_BASE=os.environ.get(\"INFINI_BASE_URL\") # will be used to pass the OpenAI base URL to langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Key abstractions in LangChain\n",
    "\n",
    "| Abstracted Components | Input Type                                | Output Type           |\n",
    "|-----------------------|-------------------------------------------|-----------------------|\n",
    "| Prompt                | Dictionary                                | PromptValue           |\n",
    "| ChatModel             | string, list of messages or a PromptValue | string, ChatMessage   |\n",
    "| OutputParser          | The output of an LLM or ChatModel         | Depends on the parser |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The ChatModel\n",
    "\n",
    "`ChatModels` is a language model which takes a list of messages or a string as input and returns a message or a string.\n",
    "\n",
    "`ChatModel` provides two methods to interact with the user:\n",
    "\n",
    "- `predict`: takes in a string, returns a string.\n",
    "- `predict_messages`: takes in a list of messages, returns a message.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChatModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "chat_model = ChatOpenAI(\n",
    "    temperature=0, \n",
    "    model=CHAT_MODEL,\n",
    "    base_url=OPENAI_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an output utility to show the type and the content of the result\n",
    "def print_with_type(res):\n",
    "    print(f\"%s\" % (type(res)))\n",
    "    print(f\"%s\" % res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four roles in LangChain, and you can define your own custom roles.\n",
    "\n",
    "- `HumanMessage`: A ChatMessage coming from a human/user.\n",
    "- `AIMessage`: A ChatMessage coming from an AI/assistant.\n",
    "- `SystemMessage`: A ChatMessage coming from the system.\n",
    "- `FunctionMessage`: A ChatMessage coming from a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content=\"Hello Xu Wei! Nice to meet you too. I'd be happy to tell you about LangChain.\\n\\nLangChain is a framework designed to help developers build applications that integrate language models into their workflows. It provides a set of tools and abstractions that make it easier to work with large language models (LLMs) like those from Hugging Face, Anthropic, and others, as well as with various data sources and APIs.\\n\\nHere are some key features and concepts in LangChain:\\n\\n1. **Chains**: These are reusable components that encapsulate common patterns for interacting with language models. For example, a chain might handle the process of summarizing text or generating responses to user queries.\\n\\n2. **Agents**: Agents are higher-level constructs that can perform more complex tasks by orchestrating multiple chains and making decisions based on the context. They can interact with external tools and APIs to fetch data, perform actions, and more.\\n\\n3. **Memory**: LangChain supports various memory mechanisms to keep track of the conversation history and context, which is crucial for building conversational agents that can maintain a coherent dialogue over multiple turns.\\n\\n4. **Data Connectors**: These allow you to connect your application to different data sources, such as databases, file systems, or web APIs, making it easy to fetch and process data as needed.\\n\\n5. **Model Integration**: LangChain provides integrations with popular language models, making it straightforward to switch between different models or use multiple models in a single application.\\n\\n6. **Tool Integration**: You can integrate external tools and services, such as search engines, databases, and other APIs, to enhance the capabilities of your application.\\n\\n7. **Modularity**: The framework is highly modular, allowing you to mix and match components to suit your specific needs. This makes it flexible and adaptable to a wide range of use cases.\\n\\n8. **Community and Ecosystem**: LangChain has a growing community of developers and contributors, which means there are many resources, examples, and plugins available to help you get started and build powerful applications.\\n\\nIf you're interested in building applications that leverage the power of language models, LangChain can be a great tool to explore. Whether you're working on chatbots, content generation, summarization, or any other NLP task, LangChain can help streamline your development process.\\n\\nFeel free to ask if you have any more questions or if there's a specific aspect of LangChain you'd like to know more about!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 494, 'prompt_tokens': 29, 'total_tokens': 523, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-a09804dc-cf1c-41be-8a82-5c41b7cffd24-0' usage_metadata={'input_tokens': 29, 'output_tokens': 494, 'total_tokens': 523, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "qtext = \"hello! my name is xu wei, nice to meet you! could you tell me something about langchain\"\n",
    "\n",
    "messages = []\n",
    "messages.append(\n",
    "    HumanMessage(content=qtext)  # construct a human message\n",
    "    )\n",
    "res = chat_model.invoke(messages)  # invoke the chat model\n",
    "\n",
    "print_with_type(res)\n",
    "\n",
    "messages.append(res)  # append the result to the chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructors are tedious to use, and you can use the following more friendly API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content=\"I'm just here to help you out! What can I assist you with today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 29, 'total_tokens': 46, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-e0f60203-f3ab-4c11-b6cf-1e558706595b-0' usage_metadata={'input_tokens': 29, 'output_tokens': 17, 'total_tokens': 46, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# a simpler way to manage messages\n",
    "from langchain.memory import ChatMessageHistory\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")  # avoid using the constructor directly\n",
    "history.add_ai_message(\"whats up?\")\n",
    "history.add_user_message(\"nothing much, you?\")\n",
    "\n",
    "res = chat_model.invoke(history.messages)\n",
    "print_with_type(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='Certainly! LangChain has a wide range of applications across various domains, thanks to its modular design and powerful integration capabilities. Here are some of the key areas where LangChain can be applied:\\n\\n### 1. **Chatbots and Conversational Agents**\\n- **Customer Support**: Building chatbots that can handle customer inquiries, provide information, and resolve issues.\\n- **Virtual Assistants**: Creating virtual assistants that can perform tasks like scheduling appointments, setting reminders, and managing to-do lists.\\n- **Educational Bots**: Developing chatbots that can assist students with learning materials, answer questions, and provide feedback.\\n\\n### 2. **Content Generation**\\n- **Article Writing**: Automating the creation of news articles, blog posts, and other written content.\\n- **Creative Writing**: Generating stories, poems, and other creative pieces.\\n- **Marketing Content**: Producing marketing copy, social media posts, and email campaigns.\\n\\n### 3. **Data Analysis and Reporting**\\n- **Data Interpretation**: Using LLMs to analyze and interpret complex data sets, providing insights and summaries.\\n- **Report Generation**: Automating the creation of detailed reports and analyses based on data inputs.\\n\\n### 4. **Code Generation and Assistance**\\n- **Code Snippets**: Generating code snippets for common programming tasks.\\n- **Full Functions**: Creating entire functions or modules based on natural language descriptions.\\n- **Code Review**: Providing suggestions and feedback on existing code.\\n\\n### 5. **Personalized Recommendations**\\n- **Product Recommendations**: Offering personalized product recommendations based on user preferences and behavior.\\n- **Content Recommendations**: Suggesting articles, videos, and other content tailored to individual interests.\\n\\n### 6. **Language Translation and Localization**\\n- **Translation Services**: Translating text from one language to another.\\n- **Localization**: Adapting content to local contexts and cultural norms.\\n\\n### 7. **Natural Language Understanding (NLU) and Processing (NLP)**\\n- **Sentiment Analysis**: Analyzing the sentiment of text data to understand customer opinions and feedback.\\n- **Entity Recognition**: Identifying and categorizing entities in text, such as names, locations, and organizations.\\n- **Text Summarization**: Generating concise summaries of long documents or articles.\\n\\n### 8. **Interactive Storytelling and Gaming**\\n- **Narrative Generation**: Creating dynamic and interactive narratives for games and storytelling applications.\\n- **Game Dialogue**: Generating realistic and contextually appropriate dialogue for game characters.\\n\\n### 9. **Legal and Compliance**\\n- **Contract Review**: Assisting lawyers in reviewing and summarizing legal documents.\\n- **Compliance Checking**: Ensuring that documents and processes comply with regulatory requirements.\\n\\n### 10. **Healthcare and Medical Applications**\\n- **Symptom Analysis**: Helping patients understand potential health issues based on symptom descriptions.\\n- **Medical Research**: Summarizing and analyzing medical research papers and clinical trials.\\n\\n### 11. **Financial Services**\\n- **Risk Assessment**: Analyzing financial data to assess risks and provide recommendations.\\n- **Investment Advice**: Offering personalized investment advice based on market trends and user profiles.\\n\\n### 12. **Research and Development**\\n- **Literature Review**: Automating the process of reviewing and summarizing scientific literature.\\n- **Hypothesis Generation**: Generating hypotheses and research questions based on existing data.\\n\\n### Example Scenarios:\\n- **Customer Service Chatbot**: A company uses LangChain to build a chatbot that can handle customer inquiries, provide product information, and escalate complex issues to human agents.\\n- **Automated News Article Generator**: A news organization uses LangChain to generate daily news articles based on press releases and other data sources.\\n- **Code Assistant**: A software development team uses LangChain to create a tool that generates code snippets and provides suggestions for improving existing code.\\n\\nThese are just a few examples of the many applications of LangChain. Its flexibility and powerful integration capabilities make it a valuable tool for a wide range of projects and industries. If you have a specific use case in mind, feel free to share it, and I can provide more tailored advice!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 827, 'prompt_tokens': 532, 'total_tokens': 1359, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-29865823-ca75-40a1-9ee8-782672b48722-0' usage_metadata={'input_tokens': 532, 'output_tokens': 827, 'total_tokens': 1359, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# remembering the chat history and context\n",
    "\n",
    "qtext = \"what is its application?\"\n",
    "messages.append(HumanMessage(content=qtext))  ## providing context of chat histroy\n",
    "res = chat_model.invoke(messages)\n",
    "print_with_type(res)\n",
    "messages.append(res)  ## remembers the histroy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prompt templates\n",
    "\n",
    "LangChain provides ``PromptTemplate`` to help with formatting the prompts. \n",
    "A ``PromptTemplate`` allows you to define a template with placeholders that can be filled in with specific values at runtime. \n",
    "This helps in creating dynamic and reusable prompts for different contexts and inputs.\n",
    "\n",
    "The most plain prompt is in the type of a ``string``. Usually, the prompt includes several different type of `Messages`, which contains the `role` and the plain prompt as `content`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Simple template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "What is a good name for a company that makes candies?\n"
     ]
    }
   ],
   "source": [
    "# Prompt Template\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "input_prompt = prompt.format(product=\"candies\")\n",
    "\n",
    "print_with_type(input_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Chinese.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Chat Template (a list of temlates in a chat prompt template)\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "# format chat message prompt\n",
    "sys_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", sys_template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "chat_input = chat_prompt.format_messages(input_language=\"English\", output_language=\"Chinese\", text=\"I love programming.\")\n",
    "\n",
    "print_with_type(chat_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Using template in the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Chinese.', additional_kwargs={}, response_metadata={}), HumanMessage(content='hello! my name is xu wei, nice to meet you! could you tell me something about langchain', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='你好！我是徐伟，很高兴见到你！当然可以，LangChain 是一个用于开发语言模型应用的框架。它提供了一系列工具和库，帮助开发者更高效地构建、训练和部署基于语言模型的应用程序。这些应用程序可以包括聊天机器人、文本生成器、翻译工具等。LangChain 支持多种流行的预训练模型，并且提供了灵活的接口来定制化你的应用。如果你有任何具体的问题或者想了解的内容，欢迎随时告诉我！' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 45, 'total_tokens': 143, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-4a3c3b15-a6e6-4c15-88f6-8bb44107dca1-0' usage_metadata={'input_tokens': 45, 'output_tokens': 98, 'total_tokens': 143, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# format messages with PromptTemplate with translator as an example\n",
    "messages = []\n",
    "chat_input = chat_prompt.format_messages(input_language=\"English\", output_language=\"Chinese\", text=qtext)\n",
    "print_with_type(chat_input)\n",
    "print_with_type(chat_model.invoke(chat_input))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Chinese.', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is its application?', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='它的应用是什么？' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 29, 'total_tokens': 33, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-59bea7d9-b2cc-4e94-bdbf-11489bdd568e-0' usage_metadata={'input_tokens': 29, 'output_tokens': 4, 'total_tokens': 33, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "messages = chat_input + messages  ## the system message must be at the beginning\n",
    "print_with_type(messages)\n",
    "\n",
    "res = chat_model.invoke(messages)\n",
    "print_with_type(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Chaining Components together\n",
    "\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components. \n",
    "In LangChain, most of the above key abstraction components are `Runnable` objects, and we can **chain** them together to build awesome applications. \n",
    "\n",
    "LangChain makes the chainning powerful through **LangChain Expression Language (LCEL)**, which can support chainning in manners of:\n",
    "\n",
    "- Async, Batch, and Streaming Support: any chain constructed in LCEL can automatically have full synv, async, batch and streaming support. \n",
    "- Fallbacks: due to many factors like network connection or non-deterministic properties, your LLM applications need to handle errors gracefully. With LCEL, your can easily attach fallbacks any chain.\n",
    "- Parallelism: since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
    "- LangSmith Tracing Integration: (for debugging, see below).\n",
    "\n",
    "In this lab, we only demonstrate the simplest functional chainning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='{\"这是传递给聊天模型链和聊天提示的输入。\"}' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 39, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-51ce98e6-8ac9-49d4-b775-bba9bf237e4f-0' usage_metadata={'input_tokens': 39, 'output_tokens': 14, 'total_tokens': 53, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# More abstractions: bundling prompt and the chat_model into a chain\n",
    "\n",
    "translate_chain = chat_prompt | chat_model\n",
    "qtext = \"this is input to a chain of chat model and chat prompt.\"\n",
    "out = translate_chain.invoke({\n",
    "    \"input_language\": \"English\", \n",
    "    \"output_language\": \"Chinese\", \n",
    "    \"text\": {qtext}\n",
    "    })\n",
    "print_with_type(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Output parser\n",
    "\n",
    "Language models output text, which is often unstructured and free-form. However, in many applications, you may need more structured information to work with, such as JSON, XML, or other formats. This is where output parsers come in.\n",
    "\n",
    "Output parsers are tools that transform the raw text output of language models into structured data formats. The motivation for using output parsers is to facilitate easier data manipulation, integration, and analysis. By converting text into structured formats, you can more effectively utilize the information in downstream applications, automate workflows, and ensure consistency in data handling.\n",
    "\n",
    "LangChain provides several commonly-used output parsers, including:\n",
    "- [JSONparser](https://python.langchain.com/docs/how_to/output_parser_json/): Converts text output into JSON format.\n",
    "- [XMLparser](https://python.langchain.com/docs/how_to/output_parser_xml/): Converts text output into XML format.\n",
    "- [YAMLparser](https://python.langchain.com/docs/how_to/output_parser_yaml/): Converts text output into YAML format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Simple output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "{\"这是传递给聊天模型和聊天提示链的输入。\"}\n"
     ]
    }
   ],
   "source": [
    "# a simple output parser\n",
    "# StdOutParser converts the chat message to a string.\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "stdoutchain = chat_prompt | chat_model | output_parser\n",
    "\n",
    "qtext = \"this is input to a chain of chat model and chat prompt.\"\n",
    "out=stdoutchain.invoke({\n",
    "    \"input_language\": \"English\", \n",
    "    \"output_language\": \"Chinese\", \n",
    "    \"text\": {qtext}\n",
    "    })\n",
    "print_with_type(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Advanced output parsers: from Results to a Python Object\n",
    "Here we demonstrate the powerful Json Outputparser as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'name': 'Wei Xu', 'publications': 'A Survey on Deep Learning for Trustworthy Recommendation, Deep Learning-based Recommender System: A Survey, Deep Learning for Social Media Analysis: A Survey'}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Professor(BaseModel):\n",
    "    name: str = Field(description=\"name of the Professor\")\n",
    "    publications: str = Field(description=\"the string of the professor's publications, separated by comma\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Professor)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "professor_chain = prompt | chat_model | parser\n",
    "query = \"tell me about professor Wei Xu including his publications.\"\n",
    "output = professor_chain.invoke({\n",
    "    \"query\": {query}\n",
    "    })\n",
    "print_with_type(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'$defs': {'AIMessage': {'additionalProperties': True, 'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'ai', 'default': 'ai', 'enum': ['ai'], 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}, 'example': {'default': False, 'title': 'Example', 'type': 'boolean'}, 'tool_calls': {'default': [], 'items': {'$ref': '#/$defs/ToolCall'}, 'title': 'Tool Calls', 'type': 'array'}, 'invalid_tool_calls': {'default': [], 'items': {'$ref': '#/$defs/InvalidToolCall'}, 'title': 'Invalid Tool Calls', 'type': 'array'}, 'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'}, {'type': 'null'}], 'default': None}}, 'required': ['content'], 'title': 'AIMessage', 'type': 'object'}, 'AIMessageChunk': {'additionalProperties': True, 'description': 'Message chunk from an AI.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'AIMessageChunk', 'default': 'AIMessageChunk', 'enum': ['AIMessageChunk'], 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}, 'example': {'default': False, 'title': 'Example', 'type': 'boolean'}, 'tool_calls': {'default': [], 'items': {'$ref': '#/$defs/ToolCall'}, 'title': 'Tool Calls', 'type': 'array'}, 'invalid_tool_calls': {'default': [], 'items': {'$ref': '#/$defs/InvalidToolCall'}, 'title': 'Invalid Tool Calls', 'type': 'array'}, 'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'}, {'type': 'null'}], 'default': None}, 'tool_call_chunks': {'default': [], 'items': {'$ref': '#/$defs/ToolCallChunk'}, 'title': 'Tool Call Chunks', 'type': 'array'}}, 'required': ['content'], 'title': 'AIMessageChunk', 'type': 'object'}, 'ChatMessage': {'additionalProperties': True, 'description': 'Message that can be assigned an arbitrary speaker (i.e. role).', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'chat', 'default': 'chat', 'enum': ['chat'], 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}, 'role': {'title': 'Role', 'type': 'string'}}, 'required': ['content', 'role'], 'title': 'ChatMessage', 'type': 'object'}, 'ChatMessageChunk': {'additionalProperties': True, 'description': 'Chat Message chunk.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'ChatMessageChunk', 'default': 'ChatMessageChunk', 'enum': ['ChatMessageChunk'], 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}, 'role': {'title': 'Role', 'type': 'string'}}, 'required': ['content', 'role'], 'title': 'ChatMessageChunk', 'type': 'object'}, 'ChatPromptValueConcrete': {'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.', 'properties': {'messages': {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'}, {'$ref': '#/$defs/HumanMessage'}, {'$ref': '#/$defs/ChatMessage'}, {'$ref': '#/$defs/SystemMessage'}, {'$ref': '#/$defs/FunctionMessage'}, {'$ref': '#/$defs/ToolMessage'}, {'$ref': '#/$defs/AIMessageChunk'}, {'$ref': '#/$defs/HumanMessageChunk'}, {'$ref': '#/$defs/ChatMessageChunk'}, {'$ref': '#/$defs/SystemMessageChunk'}, {'$ref': '#/$defs/FunctionMessageChunk'}, {'$ref': '#/$defs/ToolMessageChunk'}]}, 'title': 'Messages', 'type': 'array'}, 'type': {'const': 'ChatPromptValueConcrete', 'default': 'ChatPromptValueConcrete', 'enum': ['ChatPromptValueConcrete'], 'title': 'Type', 'type': 'string'}}, 'required': ['messages'], 'title': 'ChatPromptValueConcrete', 'type': 'object'}, 'FunctionMessage': {'additionalProperties': True, 'description': 'Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'function', 'default': 'function', 'enum': ['function'], 'title': 'Type', 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}}, 'required': ['content', 'name'], 'title': 'FunctionMessage', 'type': 'object'}, 'FunctionMessageChunk': {'additionalProperties': True, 'description': 'Function Message chunk.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'FunctionMessageChunk', 'default': 'FunctionMessageChunk', 'enum': ['FunctionMessageChunk'], 'title': 'Type', 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}}, 'required': ['content', 'name'], 'title': 'FunctionMessageChunk', 'type': 'object'}, 'HumanMessage': {'additionalProperties': True, 'description': 'Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'human', 'default': 'human', 'enum': ['human'], 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}, 'example': {'default': False, 'title': 'Example', 'type': 'boolean'}}, 'required': ['content'], 'title': 'HumanMessage', 'type': 'object'}, 'HumanMessageChunk': {'additionalProperties': True, 'description': 'Human Message chunk.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'HumanMessageChunk', 'default': 'HumanMessageChunk', 'enum': ['HumanMessageChunk'], 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}, 'example': {'default': False, 'title': 'Example', 'type': 'boolean'}}, 'required': ['content'], 'title': 'HumanMessageChunk', 'type': 'object'}, 'InputTokenDetails': {'description': 'Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"cache_creation\": 200,\\n            \"cache_read\": 100,\\n        }\\n\\n.. versionadded:: 0.3.9', 'properties': {'audio': {'title': 'Audio', 'type': 'integer'}, 'cache_creation': {'title': 'Cache Creation', 'type': 'integer'}, 'cache_read': {'title': 'Cache Read', 'type': 'integer'}}, 'title': 'InputTokenDetails', 'type': 'object'}, 'InvalidToolCall': {'description': 'Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)', 'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Name'}, 'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'}, 'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Error'}, 'type': {'const': 'invalid_tool_call', 'enum': ['invalid_tool_call'], 'title': 'Type', 'type': 'string'}}, 'required': ['name', 'args', 'id', 'error'], 'title': 'InvalidToolCall', 'type': 'object'}, 'OutputTokenDetails': {'description': 'Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"reasoning\": 200,\\n        }\\n\\n.. versionadded:: 0.3.9', 'properties': {'audio': {'title': 'Audio', 'type': 'integer'}, 'reasoning': {'title': 'Reasoning', 'type': 'integer'}}, 'title': 'OutputTokenDetails', 'type': 'object'}, 'StringPromptValue': {'description': 'String prompt value.', 'properties': {'text': {'title': 'Text', 'type': 'string'}, 'type': {'const': 'StringPromptValue', 'default': 'StringPromptValue', 'enum': ['StringPromptValue'], 'title': 'Type', 'type': 'string'}}, 'required': ['text'], 'title': 'StringPromptValue', 'type': 'object'}, 'SystemMessage': {'additionalProperties': True, 'description': 'Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'system', 'default': 'system', 'enum': ['system'], 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}}, 'required': ['content'], 'title': 'SystemMessage', 'type': 'object'}, 'SystemMessageChunk': {'additionalProperties': True, 'description': 'System Message chunk.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'SystemMessageChunk', 'default': 'SystemMessageChunk', 'enum': ['SystemMessageChunk'], 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}}, 'required': ['content'], 'title': 'SystemMessageChunk', 'type': 'object'}, 'ToolCall': {'description': 'Represents a request to call a tool.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"name\": \"foo\",\\n            \"args\": {\"a\": 1},\\n            \"id\": \"123\"\\n        }\\n\\n    This represents a request to call the tool named \"foo\" with arguments {\"a\": 1}\\n    and an identifier of \"123\".', 'properties': {'name': {'title': 'Name', 'type': 'string'}, 'args': {'title': 'Args', 'type': 'object'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'}, 'type': {'const': 'tool_call', 'enum': ['tool_call'], 'title': 'Type', 'type': 'string'}}, 'required': ['name', 'args', 'id'], 'title': 'ToolCall', 'type': 'object'}, 'ToolCallChunk': {'description': 'A chunk of a tool call (e.g., as part of a stream).\\n\\nWhen merging ToolCallChunks (e.g., via AIMessageChunk.__add__),\\nall string attributes are concatenated. Chunks are only merged if their\\nvalues of `index` are equal and not None.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    left_chunks = [ToolCallChunk(name=\"foo\", args=\\'{\"a\":\\', index=0)]\\n    right_chunks = [ToolCallChunk(name=None, args=\\'1}\\', index=0)]\\n\\n    (\\n        AIMessageChunk(content=\"\", tool_call_chunks=left_chunks)\\n        + AIMessageChunk(content=\"\", tool_call_chunks=right_chunks)\\n    ).tool_call_chunks == [ToolCallChunk(name=\\'foo\\', args=\\'{\"a\":1}\\', index=0)]', 'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Name'}, 'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'}, 'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}], 'title': 'Index'}, 'type': {'const': 'tool_call_chunk', 'enum': ['tool_call_chunk'], 'title': 'Type', 'type': 'string'}}, 'required': ['name', 'args', 'id', 'index'], 'title': 'ToolCallChunk', 'type': 'object'}, 'ToolMessage': {'additionalProperties': True, 'description': 'Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A ToolMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content=\\'42\\', tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\')\\n\\n\\nExample: A ToolMessage where only part of the tool output is sent to the model\\n    and the full output is passed in to artifact.\\n\\n    .. versionadded:: 0.2.17\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        tool_output = {\\n            \"stdout\": \"From the graph we can see that the correlation between x and y is ...\",\\n            \"stderr\": None,\\n            \"artifacts\": {\"type\": \"image\", \"base64_data\": \"/9j/4gIcSU...\"},\\n        }\\n\\n        ToolMessage(\\n            content=tool_output[\"stdout\"],\\n            artifact=tool_output,\\n            tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\',\\n        )\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'tool', 'default': 'tool', 'enum': ['tool'], 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}, 'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}, 'artifact': {'default': None, 'title': 'Artifact'}, 'status': {'default': 'success', 'enum': ['success', 'error'], 'title': 'Status', 'type': 'string'}}, 'required': ['content', 'tool_call_id'], 'title': 'ToolMessage', 'type': 'object'}, 'ToolMessageChunk': {'additionalProperties': True, 'description': 'Tool Message chunk.', 'properties': {'content': {'anyOf': [{'type': 'string'}, {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}, 'type': 'array'}], 'title': 'Content'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'response_metadata': {'title': 'Response Metadata', 'type': 'object'}, 'type': {'const': 'ToolMessageChunk', 'default': 'ToolMessageChunk', 'enum': ['ToolMessageChunk'], 'title': 'Type', 'type': 'string'}, 'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Name'}, 'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Id'}, 'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}, 'artifact': {'default': None, 'title': 'Artifact'}, 'status': {'default': 'success', 'enum': ['success', 'error'], 'title': 'Status', 'type': 'string'}}, 'required': ['content', 'tool_call_id'], 'title': 'ToolMessageChunk', 'type': 'object'}, 'UsageMetadata': {'description': 'Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"input_tokens\": 350,\\n            \"output_tokens\": 240,\\n            \"total_tokens\": 590,\\n            \"input_token_details\": {\\n                \"audio\": 10,\\n                \"cache_creation\": 200,\\n                \"cache_read\": 100,\\n            },\\n            \"output_token_details\": {\\n                \"audio\": 10,\\n                \"reasoning\": 200,\\n            }\\n        }\\n\\n.. versionchanged:: 0.3.9\\n\\n    Added ``input_token_details`` and ``output_token_details``.', 'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'}, 'output_tokens': {'title': 'Output Tokens', 'type': 'integer'}, 'total_tokens': {'title': 'Total Tokens', 'type': 'integer'}, 'input_token_details': {'$ref': '#/$defs/InputTokenDetails'}, 'output_token_details': {'$ref': '#/$defs/OutputTokenDetails'}}, 'required': ['input_tokens', 'output_tokens', 'total_tokens'], 'title': 'UsageMetadata', 'type': 'object'}}, 'anyOf': [{'type': 'string'}, {'$ref': '#/$defs/StringPromptValue'}, {'$ref': '#/$defs/ChatPromptValueConcrete'}, {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'}, {'$ref': '#/$defs/HumanMessage'}, {'$ref': '#/$defs/ChatMessage'}, {'$ref': '#/$defs/SystemMessage'}, {'$ref': '#/$defs/FunctionMessage'}, {'$ref': '#/$defs/ToolMessage'}, {'$ref': '#/$defs/AIMessageChunk'}, {'$ref': '#/$defs/HumanMessageChunk'}, {'$ref': '#/$defs/ChatMessageChunk'}, {'$ref': '#/$defs/SystemMessageChunk'}, {'$ref': '#/$defs/FunctionMessageChunk'}, {'$ref': '#/$defs/ToolMessageChunk'}]}, 'type': 'array'}], 'title': 'ChatOpenAIInput'}\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# see how langchain organizes the input to construct the result.\n",
    "# you can do so by printing the input of the chat_model.\n",
    "print_with_type(chat_model.get_input_jsonschema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the list of papers lacks substantial information and contains many inaccuracies. \n",
    "\n",
    "This is because the model has no knowlege about Prof. Wei Xu.  \n",
    "\n",
    "We will now demonstrate how to address these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Adding more contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Allowing the model to search the web: Retrievers\n",
    "\n",
    "Many LLM applications require user-specific data that is not part of the model's training set, like the above example : )\n",
    "The primary way of accomplishing this is through **Retrieval Augmented Generation (RAG)**. In this process, external data is retrieved and then passed to the LLM when doing the generation step. `Retriever` is an interface that returns documents given an unstructured query, which is used to provide the related contents to LLMs\n",
    "\n",
    "LangChain provides all the building blocks for RAG applications - from simple to complex, including document loaders, text embedding models and web searches.  We will introduce these models in Lab 4.  Here, we introduce three very basic retrievers that does web search and some local file access.  \n",
    "\n",
    "- [web page](https://python.langchain.com/docs/how_to/document_loader_web/)\n",
    "- [load Json](https://python.langchain.com/docs/how_to/document_loader_json/)\n",
    "- [load PDF files](https://python.langchain.com/docs/how_to/document_loader_pdf/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SerpAPI is a widely used API to access search engine results. It allows you to scrape and parse search engine data, providing a way to retrieve up-to-date information from the web.\n",
    "\n",
    "In LangChain, SerpAPI can be integrated as a retriever to enhance the capabilities of language models by providing them with the latest information from the web. This is particularly useful for applications that require current data that is not part of the model's training set.\n",
    "\n",
    "By using SerpAPI with LangChain, you can perform web searches and feed the retrieved data into the language model to generate more accurate and contextually relevant responses.\n",
    "\n",
    "In the following sections, we will demonstrate how to set up and use SerpAPI within LangChain to perform web searches and integrate the results into your language model workflows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'title': 'NVIDIA DGX Spark Arrives for World’s AI Developers', 'link': 'https://nvidianews.nvidia.com/news/nvidia-dgx-spark-arrives-for-worlds-ai-developers', 'source': 'NVIDIA Newsroom', 'source_logo': 'https://serpapi.com/searches/68ee3234eb2bd1084352ed80/images/1facaa3a5b60e24b5fef4458f11d580e47b71da46047b6f88670150a7df2dbf5.png', 'date': '12 hours ago', 'thumbnail': 'https://serpapi.com/searches/68ee3234eb2bd1084352ed80/images/1facaa3a5b60e24b43c2f81805cf6b5b3be5815be8f76320.jpeg'}, {'title': 'Elon Musk Gets Just-Launched NVIDIA DGX Spark: Petaflop AI Supercomputer Lands at SpaceX', 'link': 'https://blogs.nvidia.com/blog/live-dgx-spark-delivery/', 'source': 'NVIDIA Blog', 'source_logo': 'https://serpapi.com/searches/68ee3234eb2bd1084352ed80/images/1facaa3a5b60e24beb289f690a0d6f0d88a3a365efb28929fe4b571d7ae4fb18.png', 'date': '5 hours ago', 'thumbnail': 'https://serpapi.com/searches/68ee3234eb2bd1084352ed80/images/1facaa3a5b60e24b4c7b0aaf8781d047769a910f60ccdbb5.jpeg'}, {'title': 'Nvidia starts shipping $3,999 Blackwell-based desktop supercomputer', 'link': 'https://www.sdxcentral.com/news/nvidia-starts-shipping-3999-blackwell-based-desktop-supercomputer/', 'source': 'SDxCentral', 'source_logo': 'https://serpapi.com/searches/68ee3234eb2bd1084352ed80/images/1facaa3a5b60e24b24eccfedc3f6477ceef961cb837078dd7742be8be9ceb859.png', 'date': '23 minutes ago', 'thumbnail': 'https://serpapi.com/searches/68ee3234eb2bd1084352ed80/images/1facaa3a5b60e24bc3f521adf0ee9f31107994e50c988a68.jpeg'}, {'title': 'Navitas Semiconductor Stock (NVTS): Soars on Nvidia Deal—But Can It Deliver?', 'link': 'https://www.valuethemarkets.com/analysis/navitas-semiconductor-stock-nvts-ai-data-center-boost', 'source': 'Value The Markets', 'source_logo': 'https://serpapi.com/searches/68ee3234eb2bd1084352ed80/images/1facaa3a5b60e24b3f5cc9ca76340abf28d444fc87a37fce3180126d3d419161.png', 'date': '37 minutes ago', 'thumbnail': 'https://serpapi.com/searches/68ee3234eb2bd1084352ed80/images/1facaa3a5b60e24bfae7f5d60d411131c3c58ec69e5546a1.jpeg'}, {'title': \"Nvidia 800 VDC power rollout for 1 megawatt server racks to be supported by ABB — company says collaboration will create 'new power solutions for future gigawatt-scale data centers '\", 'link': 'https://www.tomshardware.com/tech-industry/big-tech/nvidia-800-vdc-power-rollout-for-1-megawatt-server-racks-to-be-supported-by-abb-company-says-collaboration-will-create-new-power-solutions-for-future-gigawatt-scale-data-centers', 'source': \"Tom's Hardware\", 'source_logo': 'https://serpapi.com/searches/68ee3234eb2bd1084352ed80/images/1facaa3a5b60e24b69e1a1e8aed316bd7e3fe2a81616e1db9dadee78418749c9.png', 'date': '1 hour ago', 'thumbnail': 'https://serpapi.com/searches/68ee3234eb2bd1084352ed80/images/1facaa3a5b60e24babd727336341ea3db8d3f4b7e073af98.jpeg'}, {'title': 'Piper Sandler reiterates Overweight rating on Nvidia stock amid robust demand', 'link': 'https://www.investing.com/news/analyst-ratings/piper-sandler-reiterates-overweight-rating-on-nvidia-stock-amid-robust-demand-93CH-4285600', 'source': 'Investing.com', 'source_logo': 'https://serpapi.com/searches/68ee3234eb2bd1084352ed80/images/1facaa3a5b60e24b75ed971bfdec17d530d4dd0952c00efa6d02d219db68ae25.png', 'date': '3 minutes ago', 'thumbnail': 'https://serpapi.com/searches/68ee3234eb2bd1084352ed80/images/1facaa3a5b60e24b66fb3dbf7d1dd2bb5aaa3616b9d0b852.jpeg'}, {'title': 'TSMC Just Shared Fantastic News for Nvidia and Broadcom Investors', 'link': 'https://www.fool.com/investing/2025/10/14/tsmc-just-shared-fantastic-news-nvidia-broadcom/', 'source': 'The Motley Fool', 'source_logo': 'https://serpapi.com/searches/68ee3234eb2bd1084352ed80/images/1facaa3a5b60e24bebf192847f989b48e4881b44e365661eedd8ee701d05c5d4.png', 'date': '51 minutes ago', 'thumbnail': 'https://serpapi.com/searches/68ee3234eb2bd1084352ed80/images/1facaa3a5b60e24b8138a12753d5821b3cc4677d8b5018de.jpeg'}]\n"
     ]
    }
   ],
   "source": [
    "# Using the search API\n",
    "\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "results = search.run(\"Nvidia\")\n",
    "print_with_type(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the search and LLM together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the nvidia companies, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the nvidia companies, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:run] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the nvidia companies, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the nvidia companies, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:RunnablePassthrough] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"tell me about the nvidia companies, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:run] [275ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"['Founded in 1993, NVIDIA is the world leader in accelerated computing. Our invention of the GPU in 1999 sparked the growth of the PC gaming market,.', 'Nvidia Corporation is an American technology company headquartered in Santa Clara, California. Founded in 1993 by Jensen Huang (president and CEO), ...', 'NVIDIA engineers the most advanced chips, systems, and software for the AI factories of the future. We build new AI services that help companies create their ...', 'NVIDIA Corporation (NVDA) is an American semiconductor company and a leading global manufacturer of high-end graphics processing units (GPUs).', 'Nvidia Corporation is a technology company known for designing and manufacturing graphics processing units (GPUs).', 'Company Overview. NVIDIA is engaged in manufacturing graphics processing units (GPUs) for the gaming and professional markets, as well as system-on-a-chips ...', \\\"Founded in 1993 — famously, over a meal at Denny's — Nvidia designs a special kind of programmable computer chip. For decades, Intel and ...\\\", 'Nvidia was established in 1993 with a mission to bring advanced graphics to mainstream computing. Initially focused on 3D graphics for gaming ...', 'NVIDIA invents the GPU and drives advances in AI, HPC, gaming, creative design, autonomous vehicles, and robotics.', \\\"NVIDIA Corporation provides graphics, and compute and networking solutions in the United States, Taiwan, China, and internationally. The company's Graphics ...\\\"]\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query>] [280ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": \"['Founded in 1993, NVIDIA is the world leader in accelerated computing. Our invention of the GPU in 1999 sparked the growth of the PC gaming market,.', 'Nvidia Corporation is an American technology company headquartered in Santa Clara, California. Founded in 1993 by Jensen Huang (president and CEO), ...', 'NVIDIA engineers the most advanced chips, systems, and software for the AI factories of the future. We build new AI services that help companies create their ...', 'NVIDIA Corporation (NVDA) is an American semiconductor company and a leading global manufacturer of high-end graphics processing units (GPUs).', 'Nvidia Corporation is a technology company known for designing and manufacturing graphics processing units (GPUs).', 'Company Overview. NVIDIA is engaged in manufacturing graphics processing units (GPUs) for the gaming and professional markets, as well as system-on-a-chips ...', \\\"Founded in 1993 — famously, over a meal at Denny's — Nvidia designs a special kind of programmable computer chip. For decades, Intel and ...\\\", 'Nvidia was established in 1993 with a mission to bring advanced graphics to mainstream computing. Initially focused on 3D graphics for gaming ...', 'NVIDIA invents the GPU and drives advances in AI, HPC, gaming, creative design, autonomous vehicles, and robotics.', \\\"NVIDIA Corporation provides graphics, and compute and networking solutions in the United States, Taiwan, China, and internationally. The company's Graphics ...\\\"]\",\n",
      "  \"query\": \"tell me about the nvidia companies, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"context\": \"['Founded in 1993, NVIDIA is the world leader in accelerated computing. Our invention of the GPU in 1999 sparked the growth of the PC gaming market,.', 'Nvidia Corporation is an American technology company headquartered in Santa Clara, California. Founded in 1993 by Jensen Huang (president and CEO), ...', 'NVIDIA engineers the most advanced chips, systems, and software for the AI factories of the future. We build new AI services that help companies create their ...', 'NVIDIA Corporation (NVDA) is an American semiconductor company and a leading global manufacturer of high-end graphics processing units (GPUs).', 'Nvidia Corporation is a technology company known for designing and manufacturing graphics processing units (GPUs).', 'Company Overview. NVIDIA is engaged in manufacturing graphics processing units (GPUs) for the gaming and professional markets, as well as system-on-a-chips ...', \\\"Founded in 1993 — famously, over a meal at Denny's — Nvidia designs a special kind of programmable computer chip. For decades, Intel and ...\\\", 'Nvidia was established in 1993 with a mission to bring advanced graphics to mainstream computing. Initially focused on 3D graphics for gaming ...', 'NVIDIA invents the GPU and drives advances in AI, HPC, gaming, creative design, autonomous vehicles, and robotics.', \\\"NVIDIA Corporation provides graphics, and compute and networking solutions in the United States, Taiwan, China, and internationally. The company's Graphics ...\\\"]\",\n",
      "  \"query\": \"tell me about the nvidia companies, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the user query based on the following context: \\n['Founded in 1993, NVIDIA is the world leader in accelerated computing. Our invention of the GPU in 1999 sparked the growth of the PC gaming market,.', 'Nvidia Corporation is an American technology company headquartered in Santa Clara, California. Founded in 1993 by Jensen Huang (president and CEO), ...', 'NVIDIA engineers the most advanced chips, systems, and software for the AI factories of the future. We build new AI services that help companies create their ...', 'NVIDIA Corporation (NVDA) is an American semiconductor company and a leading global manufacturer of high-end graphics processing units (GPUs).', 'Nvidia Corporation is a technology company known for designing and manufacturing graphics processing units (GPUs).', 'Company Overview. NVIDIA is engaged in manufacturing graphics processing units (GPUs) for the gaming and professional markets, as well as system-on-a-chips ...', \\\"Founded in 1993 — famously, over a meal at Denny's — Nvidia designs a special kind of programmable computer chip. For decades, Intel and ...\\\", 'Nvidia was established in 1993 with a mission to bring advanced graphics to mainstream computing. Initially focused on 3D graphics for gaming ...', 'NVIDIA invents the GPU and drives advances in AI, HPC, gaming, creative design, autonomous vehicles, and robotics.', \\\"NVIDIA Corporation provides graphics, and compute and networking solutions in the United States, Taiwan, China, and internationally. The company's Graphics ...\\\"]\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"properties\\\": {\\\"title\\\": {\\\"description\\\": \\\"title of the news\\\", \\\"title\\\": \\\"Title\\\", \\\"type\\\": \\\"string\\\"}, \\\"brief_desc\\\": {\\\"description\\\": \\\"brief descrption of the corresponding news\\\", \\\"title\\\": \\\"Brief Desc\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"title\\\", \\\"brief_desc\\\"]}\\n```\\nQuery: tell me about the nvidia companies, and write a brief summary for it\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [3.94s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n  \\\"title\\\": \\\"NVIDIA Corporation Overview\\\",\\n  \\\"brief_desc\\\": \\\"NVIDIA, founded in 1993 by Jensen Huang, is an American technology company headquartered in Santa Clara, California. It is renowned for its innovation in accelerated computing, particularly through the invention of the Graphics Processing Unit (GPU) in 1999, which revolutionized the PC gaming market. NVIDIA continues to lead in the development of high-performance GPUs and AI technologies, serving industries such as gaming, professional visualization, data centers, and automotive.\\\"\\n}\\n```\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"```json\\n{\\n  \\\"title\\\": \\\"NVIDIA Corporation Overview\\\",\\n  \\\"brief_desc\\\": \\\"NVIDIA, founded in 1993 by Jensen Huang, is an American technology company headquartered in Santa Clara, California. It is renowned for its innovation in accelerated computing, particularly through the invention of the Graphics Processing Unit (GPU) in 1999, which revolutionized the PC gaming market. NVIDIA continues to lead in the development of high-performance GPUs and AI technologies, serving industries such as gaming, professional visualization, data centers, and automotive.\\\"\\n}\\n```\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 115,\n",
      "                \"prompt_tokens\": 538,\n",
      "                \"total_tokens\": 653,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-513abee5-15af-4495-bd46-1e1b39111c9c-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 538,\n",
      "              \"output_tokens\": 115,\n",
      "              \"total_tokens\": 653,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 115,\n",
      "      \"prompt_tokens\": 538,\n",
      "      \"total_tokens\": 653,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [18ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"title\": \"NVIDIA Corporation Overview\",\n",
      "  \"brief_desc\": \"NVIDIA, founded in 1993 by Jensen Huang, is an American technology company headquartered in Santa Clara, California. It is renowned for its innovation in accelerated computing, particularly through the invention of the Graphics Processing Unit (GPU) in 1999, which revolutionized the PC gaming market. NVIDIA continues to lead in the development of high-performance GPUs and AI technologies, serving industries such as gaming, professional visualization, data centers, and automotive.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [4.25s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"title\": \"NVIDIA Corporation Overview\",\n",
      "  \"brief_desc\": \"NVIDIA, founded in 1993 by Jensen Huang, is an American technology company headquartered in Santa Clara, California. It is renowned for its innovation in accelerated computing, particularly through the invention of the Graphics Processing Unit (GPU) in 1999, which revolutionized the PC gaming market. NVIDIA continues to lead in the development of high-performance GPUs and AI technologies, serving industries such as gaming, professional visualization, data centers, and automotive.\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{'title': 'NVIDIA Corporation Overview', 'brief_desc': 'NVIDIA, founded in 1993 by Jensen Huang, is an American technology company headquartered in Santa Clara, California. It is renowned for its innovation in accelerated computing, particularly through the invention of the Graphics Processing Unit (GPU) in 1999, which revolutionized the PC gaming market. NVIDIA continues to lead in the development of high-performance GPUs and AI technologies, serving industries such as gaming, professional visualization, data centers, and automotive.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "class News(BaseModel):\n",
    "    title: str = Field(description=\"title of the news\")\n",
    "    brief_desc: str = Field(description=\"brief descrption of the corresponding news\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=News)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query based on the following context: \\n{context}\\n{format_instructions}\\nQuery: {query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chat_model.temperature = 0\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "setup_and_retrieval = {\n",
    "        \"context\": search.run,  # passing a retriever\n",
    "        \"query\": RunnablePassthrough()\n",
    "}\n",
    "websearch_chain = setup_and_retrieval | prompt | chat_model | parser\n",
    "\n",
    "res = websearch_chain.invoke(\"tell me about the nvidia companies, and write a brief summary for it\")\n",
    "\n",
    "print_with_type(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Debugging and Logging\n",
    "\n",
    "#### 2.2.1 The debug mode\n",
    "\n",
    "LangChain provides a ``debug`` mode that allows you to see detailed information about the execution of your chains and agents. When debug mode is enabled, LangChain will print verbose output showing:\n",
    "\n",
    "1. The exact prompts being sent to the LLM\n",
    "2. The raw responses received from the LLM\n",
    "3. The execution flow of chains and agents\n",
    "4. Any intermediate steps and tool calls\n",
    "\n",
    "This is extremely useful for:\n",
    "- Troubleshooting unexpected outputs\n",
    "- Understanding how your chains are processing data\n",
    "- Optimizing prompts\n",
    "- Identifying errors in your chain's logic\n",
    "\n",
    "You can enable debug mode using `set_debug(True)` and disable it with `set_debug(False)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:run] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:run] [302ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"['NVIDIA engineers the most advanced chips, systems, and software for the AI factories of the future. We build new AI services that help companies create their ...', 'Founded in 1993, NVIDIA is the world leader in accelerated computing. Our invention of the GPU in 1999 sparked the growth of the PC gaming market,.', 'Nvidia Corporation is an American technology company headquartered in Santa Clara, California. Founded in 1993 by Jensen Huang (president and CEO), ...', 'NVIDIA Corporation (NVDA) is an American semiconductor company and a leading global manufacturer of high-end graphics processing units (GPUs).', 'Nvidia Corporation is a technology company known for designing and manufacturing graphics processing units (GPUs).', 'NVIDIA Corporation, a computing infrastructure company, provides graphics and compute and networking solutions in the United States, Singapore, Taiwan, China, ...', 'NVIDIA Corp (NVIDIA) is a designer and developer of graphics processing units, central processing units, and system-on-a-chip units.', 'The California-based chip designer, whose products are manufactured in Taiwan, is helping lay the foundation for the AI revolution.', 'NVIDIA Corp. engages in the design and manufacture of computer graphics processors, chipsets, and related multimedia software.', 'NVIDIA is engaged in manufacturing graphics processing units (GPUs) for the gaming and professional markets, as well as system-on-a-chips (SoCs) for mobile ...']\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query>] [307ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": \"['NVIDIA engineers the most advanced chips, systems, and software for the AI factories of the future. We build new AI services that help companies create their ...', 'Founded in 1993, NVIDIA is the world leader in accelerated computing. Our invention of the GPU in 1999 sparked the growth of the PC gaming market,.', 'Nvidia Corporation is an American technology company headquartered in Santa Clara, California. Founded in 1993 by Jensen Huang (president and CEO), ...', 'NVIDIA Corporation (NVDA) is an American semiconductor company and a leading global manufacturer of high-end graphics processing units (GPUs).', 'Nvidia Corporation is a technology company known for designing and manufacturing graphics processing units (GPUs).', 'NVIDIA Corporation, a computing infrastructure company, provides graphics and compute and networking solutions in the United States, Singapore, Taiwan, China, ...', 'NVIDIA Corp (NVIDIA) is a designer and developer of graphics processing units, central processing units, and system-on-a-chip units.', 'The California-based chip designer, whose products are manufactured in Taiwan, is helping lay the foundation for the AI revolution.', 'NVIDIA Corp. engages in the design and manufacture of computer graphics processors, chipsets, and related multimedia software.', 'NVIDIA is engaged in manufacturing graphics processing units (GPUs) for the gaming and professional markets, as well as system-on-a-chips (SoCs) for mobile ...']\",\n",
      "  \"query\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"context\": \"['NVIDIA engineers the most advanced chips, systems, and software for the AI factories of the future. We build new AI services that help companies create their ...', 'Founded in 1993, NVIDIA is the world leader in accelerated computing. Our invention of the GPU in 1999 sparked the growth of the PC gaming market,.', 'Nvidia Corporation is an American technology company headquartered in Santa Clara, California. Founded in 1993 by Jensen Huang (president and CEO), ...', 'NVIDIA Corporation (NVDA) is an American semiconductor company and a leading global manufacturer of high-end graphics processing units (GPUs).', 'Nvidia Corporation is a technology company known for designing and manufacturing graphics processing units (GPUs).', 'NVIDIA Corporation, a computing infrastructure company, provides graphics and compute and networking solutions in the United States, Singapore, Taiwan, China, ...', 'NVIDIA Corp (NVIDIA) is a designer and developer of graphics processing units, central processing units, and system-on-a-chip units.', 'The California-based chip designer, whose products are manufactured in Taiwan, is helping lay the foundation for the AI revolution.', 'NVIDIA Corp. engages in the design and manufacture of computer graphics processors, chipsets, and related multimedia software.', 'NVIDIA is engaged in manufacturing graphics processing units (GPUs) for the gaming and professional markets, as well as system-on-a-chips (SoCs) for mobile ...']\",\n",
      "  \"query\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the user query based on the following context: \\n['NVIDIA engineers the most advanced chips, systems, and software for the AI factories of the future. We build new AI services that help companies create their ...', 'Founded in 1993, NVIDIA is the world leader in accelerated computing. Our invention of the GPU in 1999 sparked the growth of the PC gaming market,.', 'Nvidia Corporation is an American technology company headquartered in Santa Clara, California. Founded in 1993 by Jensen Huang (president and CEO), ...', 'NVIDIA Corporation (NVDA) is an American semiconductor company and a leading global manufacturer of high-end graphics processing units (GPUs).', 'Nvidia Corporation is a technology company known for designing and manufacturing graphics processing units (GPUs).', 'NVIDIA Corporation, a computing infrastructure company, provides graphics and compute and networking solutions in the United States, Singapore, Taiwan, China, ...', 'NVIDIA Corp (NVIDIA) is a designer and developer of graphics processing units, central processing units, and system-on-a-chip units.', 'The California-based chip designer, whose products are manufactured in Taiwan, is helping lay the foundation for the AI revolution.', 'NVIDIA Corp. engages in the design and manufacture of computer graphics processors, chipsets, and related multimedia software.', 'NVIDIA is engaged in manufacturing graphics processing units (GPUs) for the gaming and professional markets, as well as system-on-a-chips (SoCs) for mobile ...']\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"properties\\\": {\\\"title\\\": {\\\"description\\\": \\\"title of the news\\\", \\\"title\\\": \\\"Title\\\", \\\"type\\\": \\\"string\\\"}, \\\"brief_desc\\\": {\\\"description\\\": \\\"brief descrption of the corresponding news\\\", \\\"title\\\": \\\"Brief Desc\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"title\\\", \\\"brief_desc\\\"]}\\n```\\nQuery: tell me about the company nvidia, and write a brief summary for it\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [6.55s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n  \\\"title\\\": \\\"NVIDIA Corporation\\\",\\n  \\\"brief_desc\\\": \\\"NVIDIA, founded in 1993 and headquartered in Santa Clara, California, is a leading technology company specializing in accelerated computing. Known for inventing the GPU in 1999, which revolutionized the PC gaming market, NVIDIA now focuses on developing advanced chips, systems, and software for artificial intelligence and high-performance computing. Their products, including GPUs and system-on-a-chip units, are used in various industries, from gaming and professional markets to mobile computing and AI infrastructure.\\\"\\n}\\n```\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"```json\\n{\\n  \\\"title\\\": \\\"NVIDIA Corporation\\\",\\n  \\\"brief_desc\\\": \\\"NVIDIA, founded in 1993 and headquartered in Santa Clara, California, is a leading technology company specializing in accelerated computing. Known for inventing the GPU in 1999, which revolutionized the PC gaming market, NVIDIA now focuses on developing advanced chips, systems, and software for artificial intelligence and high-performance computing. Their products, including GPUs and system-on-a-chip units, are used in various industries, from gaming and professional markets to mobile computing and AI infrastructure.\\\"\\n}\\n```\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 121,\n",
      "                \"prompt_tokens\": 523,\n",
      "                \"total_tokens\": 644,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a900fe4a-6afa-4534-8145-42cecd9787b9-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 523,\n",
      "              \"output_tokens\": 121,\n",
      "              \"total_tokens\": 644,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 121,\n",
      "      \"prompt_tokens\": 523,\n",
      "      \"total_tokens\": 644,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [18ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"title\": \"NVIDIA Corporation\",\n",
      "  \"brief_desc\": \"NVIDIA, founded in 1993 and headquartered in Santa Clara, California, is a leading technology company specializing in accelerated computing. Known for inventing the GPU in 1999, which revolutionized the PC gaming market, NVIDIA now focuses on developing advanced chips, systems, and software for artificial intelligence and high-performance computing. Their products, including GPUs and system-on-a-chip units, are used in various industries, from gaming and professional markets to mobile computing and AI infrastructure.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [6.89s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"title\": \"NVIDIA Corporation\",\n",
      "  \"brief_desc\": \"NVIDIA, founded in 1993 and headquartered in Santa Clara, California, is a leading technology company specializing in accelerated computing. Known for inventing the GPU in 1999, which revolutionized the PC gaming market, NVIDIA now focuses on developing advanced chips, systems, and software for artificial intelligence and high-performance computing. Their products, including GPUs and system-on-a-chip units, are used in various industries, from gaming and professional markets to mobile computing and AI infrastructure.\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{'title': 'NVIDIA Corporation', 'brief_desc': 'NVIDIA, founded in 1993 and headquartered in Santa Clara, California, is a leading technology company specializing in accelerated computing. Known for inventing the GPU in 1999, which revolutionized the PC gaming market, NVIDIA now focuses on developing advanced chips, systems, and software for artificial intelligence and high-performance computing. Their products, including GPUs and system-on-a-chip units, are used in various industries, from gaming and professional markets to mobile computing and AI infrastructure.'}\n"
     ]
    }
   ],
   "source": [
    "# Debugging and logging: debug mode\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "\n",
    "# Try rerun the previous example to see the verbose output.\n",
    "res = websearch_chain.invoke(\"tell me about the company nvidia, and write a brief summary for it\")\n",
    "\n",
    "print_with_type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'title': 'NVIDIA: Pioneering Advanced Computing and AI', 'brief_desc': 'NVIDIA, founded in 1993 and headquartered in Santa Clara, California, is a leading technology company known for its innovation in graphics processing units (GPUs) and accelerated computing. Initially sparking the growth of the PC gaming market with the invention of the GPU, NVIDIA has expanded its focus to include AI, data centers, and autonomous vehicles, positioning itself at the forefront of the AI revolution.'}\n"
     ]
    }
   ],
   "source": [
    "set_debug(False)\n",
    "# Try rerun the previous example to see the verbose output.\n",
    "res = websearch_chain.invoke(\"tell me about the company nvidia, and write a brief summary for it\")\n",
    "\n",
    "print_with_type(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Tracing with LangSmith\n",
    "\n",
    "LangSmith is a developer platform that helps you debug, test, evaluate, and monitor LLM applications. It provides:\n",
    "\n",
    "1. **Tracing**: Visualize the execution flow of your chains and agents\n",
    "2. **Debugging**: Inspect inputs, outputs, and intermediate steps\n",
    "3. **Evaluation**: Measure and compare model performance\n",
    "4. **Monitoring**: Track usage and performance in production\n",
    " \n",
    "To use LangSmith tracing:\n",
    "Set environment variables (in your .env file):\n",
    "   - `LANGCHAIN_TRACING_V2=\"true\"` to enable tracing\n",
    "   - `LANGCHAIN_API_KEY` with your LangSmith API key\n",
    "\n",
    "After running your code, you can view detailed traces at https://docs.smith.langchain.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'NVIDIA Corporation: A Leader in Accelerated Computing',\n",
       " 'brief_desc': 'NVIDIA, founded in 1993 by Jensen Huang, is a leading American technology company headquartered in Santa Clara, California. Known for inventing the GPU in 1999, NVIDIA has significantly impacted the PC gaming market and continues to innovate in areas such as AI, HPC, autonomous vehicles, and robotics. The company designs advanced chips, systems, and software, providing solutions for both consumer and professional markets.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "\n",
    "# Auto-trace LLM calls in-context\n",
    "client = wrap_openai(openai.Client())\n",
    "\n",
    "@traceable # Auto-trace this function\n",
    "def pipeline(user_input: str):\n",
    "    return websearch_chain.invoke(user_input)\n",
    "pipeline(\"tell me about the nvidia companies, and write a brief summary for it\")\n",
    "# Out:  Hello there! How can I assist you today? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your task ####\n",
    "# go to the langsmith webpage and observe the traces. \n",
    "# NVIDIA, founded in 1993 by Jensen Huang, is a leading American technology company headquartered in Santa Clara, California. Known for inventing the GPU in 1999, NVIDIA has significantly impacted the PC gaming market and continues to innovate in areas such as AI, HPC, autonomous vehicles, and robotics. The company designs advanced chips, systems, and software, providing solutions for both consumer and professional markets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'name': 'Wei Xu', 'publications': 'Deep Learning for Natural Language Processing,2015; Neural Machine Translation with Reconstruction,2016; A Convolutional Encoder-Decoder Architecture for Anomaly Detection,2018'}\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# retrieve the information and fix the query results about Prof. Xu, \n",
    "# generating the correct Professor object.\n",
    "# Note that you do not have to get a perfect answer from the LLM in this lab.  \n",
    "# (if the answer is not perfect, please analyze and debug it in the next cell.)\n",
    "from typing import List\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "class Professor(BaseModel):\n",
    "    name: str = Field(description=\"name of the Professor\")\n",
    "    publications: str = Field(description=\"the string of the professor's publications, separated by comma\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Professor)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chat_model.temperature = 0\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "setup_and_retrieval = {\n",
    "        \"context\": search.run,  # passing a retriever\n",
    "        \"query\": RunnablePassthrough()\n",
    "}\n",
    "\n",
    "professor_chain = setup_and_retrieval | prompt | chat_model | parser\n",
    "\n",
    "query = \"tell me about professor Wei Xu including his publications.\"\n",
    "output = professor_chain.invoke({\n",
    "    \"query\": {query}\n",
    "    })\n",
    "print_with_type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR TASK ####\n",
    "# analyze the answer, if the answer is not correct, write down some comments \n",
    "# about why the answer is not correct. \n",
    "# 答案看起来是错的，并没有找到这篇论文，我觉得可能是 Wei Xu 的名字太普遍，搜索结果不准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Smarter workflow: Agents\n",
    "\n",
    "\n",
    "An AI agent is an autonomous system that perceives its environment, makes decisions, and takes actions to achieve specific goals. In the context of LLMs, an agent uses a language model as its reasoning engine to determine what actions to take and in what order, unlike chains where the sequence is predefined.\n",
    "\n",
    "LangChain provides several frameworks for building agents:\n",
    "1. **Tool integration**: LangChain allows agents to use external tools and APIs to gather information or perform actions\n",
    "2. **Agent types**: Supports various agent architectures like ReAct (Reasoning and Acting), Plan-and-Execute, and others\n",
    "3. **Memory systems**: Enables agents to maintain context across interactions (next lab)\n",
    "4. **Structured output**: Helps parse and validate agent responses\n",
    "\n",
    "For more advanced agent capabilities, LangGraph (an extension of LangChain) offers enhanced features for creating highly controllable and customizable agents with better state management and complex workflows (next lab).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Function-calling: Letter r's in straberry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the following very simple example, and see if LLM can get it correct. (the correct answer is 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The word \"strawberry\" contains two \\'r\\' letters.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 19, 'total_tokens': 33, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bd28d0cf-5f3d-48b5-bcae-e41f88c08f75-0', usage_metadata={'input_tokens': 19, 'output_tokens': 14, 'total_tokens': 33, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"how many r's are there in the word strawberry?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the AI answered incorrectly by stating there are two 'r's in the word \"strawberry\" when there are actually three 'r's (st**r**awbe**rr**y). This demonstrates a limitation of LLMs in performing simple counting tasks. Even advanced models can make these basic errors because they process text holistically rather than character-by-character like humans do. This is why tools like function calling are useful - they allow us to delegate specific tasks (like counting) to dedicated functions that can perform them accurately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[StructuredTool(name='get_letter_count', description='Returns the number of a specific letter in a word. \\n    The input should be in the format: word,letter (e.g., strawberry,r)', args_schema=<class 'langchain_core.utils.pydantic.get_letter_count'>, func=<function get_letter_count at 0x7f9ee67f7e20>)]\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "# here is an example of a tool that can be used to count the number of a specific letter in a word.\n",
    "# note that we only use a single string parameter, because the simple agent only accept tools with a single parameter.\n",
    "# to fit two parameters, we need a clear format instruction for the input.\n",
    "@tool\n",
    "def get_letter_count(query: str) -> int:\n",
    "    \"\"\"Returns the number of a specific letter in a word. \n",
    "    The input should be in the format: word,letter (e.g., strawberry,r)\"\"\"\n",
    "    word, letter = query.split(',')\n",
    "    return word.count(letter)\n",
    "\n",
    "print(get_letter_count.invoke(\"strawberry,r\"))\n",
    "\n",
    "tools = [ get_letter_count ]\n",
    "\n",
    "print(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ( \"system\", \"You are very powerful assistant who can use tools, but bad at counting letters in words.\", \n",
    "         ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55368/2941073740.py:5: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_chain = initialize_agent(tools,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"how many r's are there in the word strawberry?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"how many r's are there in the word strawberry?\",\n",
      "  \"agent_scratchpad\": \"\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nget_letter_count(query: str) -> int - Returns the number of a specific letter in a word. \\n    The input should be in the format: word,letter (e.g., strawberry,r)\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [get_letter_count]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: how many r's are there in the word strawberry?\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [3.86s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I need to find out how many 'r' letters are in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I need to find out how many 'r' letters are in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 65,\n",
      "                \"prompt_tokens\": 180,\n",
      "                \"total_tokens\": 245,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a0086955-8700-4b35-894b-f422b5a2040d-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 180,\n",
      "              \"output_tokens\": 65,\n",
      "              \"total_tokens\": 245,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 65,\n",
      "      \"prompt_tokens\": 180,\n",
      "      \"total_tokens\": 245,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [3.87s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I need to find out how many 'r' letters are in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:get_letter_count] Entering Tool run with input:\n",
      "\u001b[0m\"strawberry,r\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:get_letter_count] [3ms] Exiting Tool run with output:\n",
      "\u001b[0m\"3\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"how many r's are there in the word strawberry?\",\n",
      "  \"agent_scratchpad\": \"I need to find out how many 'r' letters are in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\\nObservation: 3\\nThought:\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nget_letter_count(query: str) -> int - Returns the number of a specific letter in a word. \\n    The input should be in the format: word,letter (e.g., strawberry,r)\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [get_letter_count]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: how many r's are there in the word strawberry?\\nThought:I need to find out how many 'r' letters are in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\\nObservation: 3\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [1.40s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I now know the final answer\\nFinal Answer: There are 3 'r's in the word strawberry.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I now know the final answer\\nFinal Answer: There are 3 'r's in the word strawberry.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 22,\n",
      "                \"prompt_tokens\": 219,\n",
      "                \"total_tokens\": 241,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-924eb3ca-3659-4e40-942e-cdbdcddf289c-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 219,\n",
      "              \"output_tokens\": 22,\n",
      "              \"total_tokens\": 241,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 22,\n",
      "      \"prompt_tokens\": 219,\n",
      "      \"total_tokens\": 241,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [1.41s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I now know the final answer\\nFinal Answer: There are 3 'r's in the word strawberry.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor] [5.29s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"There are 3 'r's in the word strawberry.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "agent_chain = initialize_agent(tools, \n",
    "                               chat_model, \n",
    "                               agent=\"zero-shot-react-description\", \n",
    "                               prompt_template=prompt, \n",
    "                               verbose=False\n",
    "                               )\n",
    "\n",
    "agent_chain.invoke({\"input\": \"how many r's are there in the word strawberry?\"})\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"sort these numbers: 19, 3, 5, 8, 7, 9, 4, 3\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"sort these numbers: 19, 3, 5, 8, 7, 9, 4, 3\",\n",
      "  \"agent_scratchpad\": \"\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nsort_numbers(input: str) -> list[int] - a tool that sort an array of numbers (packed as a comma separated string).\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [sort_numbers]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: sort these numbers: 19, 3, 5, 8, 7, 9, 4, 3\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [4.75s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I need to use the sort_numbers function to sort the given numbers.\\nAction: sort_numbers\\nAction Input: 19, 3, 5, 8, 7, 9, 4, 3\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I need to use the sort_numbers function to sort the given numbers.\\nAction: sort_numbers\\nAction Input: 19, 3, 5, 8, 7, 9, 4, 3\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 116,\n",
      "                \"prompt_tokens\": 183,\n",
      "                \"total_tokens\": 299,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f4454c28-33dd-4ee0-9bae-9bebb1fdc110-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 183,\n",
      "              \"output_tokens\": 116,\n",
      "              \"total_tokens\": 299,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 116,\n",
      "      \"prompt_tokens\": 183,\n",
      "      \"total_tokens\": 299,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [4.75s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I need to use the sort_numbers function to sort the given numbers.\\nAction: sort_numbers\\nAction Input: 19, 3, 5, 8, 7, 9, 4, 3\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:sort_numbers] Entering Tool run with input:\n",
      "\u001b[0m\"19, 3, 5, 8, 7, 9, 4, 3\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:sort_numbers] [3ms] Exiting Tool run with output:\n",
      "\u001b[0m\"[3, 3, 4, 5, 7, 8, 9, 19]\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"sort these numbers: 19, 3, 5, 8, 7, 9, 4, 3\",\n",
      "  \"agent_scratchpad\": \"I need to use the sort_numbers function to sort the given numbers.\\nAction: sort_numbers\\nAction Input: 19, 3, 5, 8, 7, 9, 4, 3\\nObservation: [3, 3, 4, 5, 7, 8, 9, 19]\\nThought:\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nsort_numbers(input: str) -> list[int] - a tool that sort an array of numbers (packed as a comma separated string).\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [sort_numbers]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: sort these numbers: 19, 3, 5, 8, 7, 9, 4, 3\\nThought:I need to use the sort_numbers function to sort the given numbers.\\nAction: sort_numbers\\nAction Input: 19, 3, 5, 8, 7, 9, 4, 3\\nObservation: [3, 3, 4, 5, 7, 8, 9, 19]\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [2.63s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I now know the final answer\\nFinal Answer: The sorted numbers are 3, 3, 4, 5, 7, 8, 9, 19.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I now know the final answer\\nFinal Answer: The sorted numbers are 3, 3, 4, 5, 7, 8, 9, 19.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 39,\n",
      "                \"prompt_tokens\": 259,\n",
      "                \"total_tokens\": 298,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-b04581d1-5a29-45c7-b0c2-173c5a4964e5-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 259,\n",
      "              \"output_tokens\": 39,\n",
      "              \"total_tokens\": 298,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 39,\n",
      "      \"prompt_tokens\": 259,\n",
      "      \"total_tokens\": 298,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I now know the final answer\\nFinal Answer: The sorted numbers are 3, 3, 4, 5, 7, 8, 9, 19.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor] [7.40s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The sorted numbers are 3, 3, 4, 5, 7, 8, 9, 19.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#### your task ####\n",
    "# implement a tool that sort an array of numbers (packed as a comma separated string)\n",
    "# then ask the agent to use the tool to sort an input array. \n",
    "@tool\n",
    "def sort_numbers(input: str) -> list[int]:\n",
    "    \"\"\"a tool that sort an array of numbers (packed as a comma separated string).\"\"\"\n",
    "    nums = [int(x) for x in input.split(\", \")]\n",
    "    return sorted(nums)\n",
    "\n",
    "print(sort_numbers.invoke(\"9, 8, 7, 6, 5, 4, 3, 2, 1\"))\n",
    "tools = [ sort_numbers ]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ( \"system\", \"You are very powerful assistant who can use tools, but bad at sorting numbers\", \n",
    "         ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n",
    "    ]\n",
    ")\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "agent_chain = initialize_agent(tools, \n",
    "                               chat_model, \n",
    "                               agent=\"zero-shot-react-description\", \n",
    "                               prompt_template=prompt, \n",
    "                               verbose=False\n",
    "                               )\n",
    "\n",
    "agent_chain.invoke({\"input\": \"sort these numbers: 19, 3, 5, 8, 7, 9, 4, 3\"})\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create an auto-web-search AI Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the following example about the built-in web-search tool and understand the code. \n",
    "\n",
    "\n",
    "from langchain.agents import load_tools \n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=News)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, helping the users search the web and write summary for the user's interested topic: {keyword}\",\n",
    "        ),\n",
    "        (\"user\", \"{keyword}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "tools = [load_tools([\"serpapi\"], chat_model)[0]]\n",
    "\n",
    "agent_chain = initialize_agent(tools, chat_model, agent=\"zero-shot-react-description\", prompt_template=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find recent news about Tsinghua University.\n",
      "Action: Search\n",
      "Action Input: \"Tsinghua University news last week\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['\\u200bCelebrating 100 years of Tsinghua Academy of Chinese Learning · \\u200bThe opening ceremony of Chinese Cultural Subjectivity, World Vision: Centennial Symposium ...', \"LATEST NEWS · \\u200bTsinghua's Long Di awarded by AGU for contributions to hydrologic sciences · From Tsinghua to the field: A Tsinghua intern at UNHCR Zambia · \\u200b ...\", 'The latest news, analysis and opinion on Tsinghua University. In-depth analysis, industry insights and expert opinion.', 'A House select committee is requesting more information about a university collaboration that it said could help China gain access to cutting-edge research.', 'LATEST NEWS · \\u200bTsinghua and PKU jointly triumph at 16th S.-T. · \\u200b13th World Peace Forum opens at Tsinghua, Chinese vice president addresses the forum · \\u200b2025 ...', '(ECNS) -- Apple CEO Tim Cook on Tuesday announced a new donation to Tsinghua University aimed at supporting student development in sustainable ...', \"Influential maths expert calls to nurture domestic talent, notes 'profound symbolic weight' of Hong Kong's bid to host top 2030 conference.\", 'LATEST NEWS · Dec09. \\u200bA creative journey towards film scoring · Dec09. Thousands run in Tsinghua winter mini-marathon · Dec06. Delegation of mainland university ...', \"Tsinghua University took the top position, followed by Singapore's Nanyang Technological University, China's Peking University, Zhejiang ...\", 'The first round of undergrad and art programs applications is open now! Tsinghua offers 93 undergraduate majors & 44 minors across 12 disciplines, ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI found several pieces of news related to Tsinghua University from the past week. I will summarize the most relevant ones.\n",
      "Final Answer: Within the last week, notable news from Tsinghua University includes:\n",
      "1. Tsinghua's Long Di was awarded by the American Geophysical Union (AGU) for significant contributions to hydrologic sciences.\n",
      "2. The 13th World Peace Forum opened at Tsinghua University, where the Chinese vice president addressed the forum.\n",
      "3. Apple CEO Tim Cook announced a new donation to Tsinghua University to support student development in sustainability.\n",
      "4. A delegation from mainland universities visited Tsinghua, likely for academic and research collaborations.\n",
      "5. The first round of undergraduate and art program applications is now open at Tsinghua, offering a wide range of majors and minors across various disciplines.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me the news from tsinghua university within last week?',\n",
       " 'output': \"Within the last week, notable news from Tsinghua University includes:\\n1. Tsinghua's Long Di was awarded by the American Geophysical Union (AGU) for significant contributions to hydrologic sciences.\\n2. The 13th World Peace Forum opened at Tsinghua University, where the Chinese vice president addressed the forum.\\n3. Apple CEO Tim Cook announced a new donation to Tsinghua University to support student development in sustainability.\\n4. A delegation from mainland universities visited Tsinghua, likely for academic and research collaborations.\\n5. The first round of undergraduate and art program applications is now open at Tsinghua, offering a wide range of majors and minors across various disciplines.\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.invoke(\"tell me the news from tsinghua university within last week?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find information about Prof. Wei Xu at Tsinghua University and his publication list.\n",
      "Action: Search\n",
      "Action Input: \"Prof. Wei Xu Tsinghua University publications\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['My current projects include privacy-preserving computation, data center networking, large scale system for machine learning and data mining, as well as various ...', 'Wei Xu. Professor, IIIS and CollegeAI, Tsinghua University. Verified email at tsinghua.edu.cn - Homepage · Computer Science. ArticlesCited byPublic access ...', 'Publication Topics. Convolutional Layers,Convolutional Neural Network,Deep Learning,Deep Neural Network,Dice Similarity Coefficient,Feature Maps,Fully ...', 'Publications. Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking? Chengda Lu, Xiaoyu Fan, Yu Huang, Rongwu Xu, Jijie Li, Wei Xu.', \"Welcome to Professor Xu Wei's Research Group at the Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University!\", 'Wei XU | Cited by 811 | of Tsinghua University, Beijing (TH) | Read 29 publications | Contact Wei XU.', 'Wei Xu is an assistant professor at the Institute for Interdisciplinary Information Sciences of Tsinghua University. ... He has published 20+ research papers in ...', 'AI-assisted CT imaging analysis for COVID-19 screening: Building and deploying a medical AI system. B Wang, S Jin, Q Yan, H Xu, C Luo, L Wei, W Zhao, X Hou, W ...', \"Wei Xu's 4 research works with 77 citations, including: Scalable Kernel TCP Design and Implementation for Short-Lived Connections.\", 'Tsinghua University. Assistant Professor at Institute for Interdisciplinary Information Sciences IIIS. Research Area: Distributed Systems + Machine Learning.']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know some details about Prof. Wei Xu at Tsinghua University and his research interests and publications.\n",
      "Final Answer: Prof. Wei Xu is a faculty member at the Institute for Interdisciplinary Information Sciences (IIIS) at Tsinghua University. His research interests include privacy-preserving computation, data center networking, large-scale systems for machine learning and data mining, and more. Some of his notable publications include works on AI-assisted CT imaging analysis for COVID-19 screening and scalable kernel TCP design for short-lived connections. He has also contributed to the field with several other research papers and is actively involved in advancing the areas of distributed systems and machine learning.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me the something about prof. wei xu in tsinghua university and his publication list',\n",
       " 'output': 'Prof. Wei Xu is a faculty member at the Institute for Interdisciplinary Information Sciences (IIIS) at Tsinghua University. His research interests include privacy-preserving computation, data center networking, large-scale systems for machine learning and data mining, and more. Some of his notable publications include works on AI-assisted CT imaging analysis for COVID-19 screening and scalable kernel TCP design for short-lived connections. He has also contributed to the field with several other research papers and is actively involved in advancing the areas of distributed systems and machine learning.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# use the web search tool to find out about \n",
    "# prof. wei xu and his publication list.  \n",
    "# Compare the results with the previous implementation. is it better or worse?\n",
    "agent_chain.invoke(\"tell me the something about prof. wei xu in tsinghua university and his publication list\")\n",
    "# It is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Use one of the built-in tool for langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to first find out who the president of the USA is or will be in 2025, and then I'll need to find out who their spouse is. Since these are future events, I'll use the Search tool to get the most up-to-date information.\n",
      "Action: Search\n",
      "Action Input: \"president of the USA in 2025\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[{'title': '‘Who was president in 2020?’ Trump and GOP want you to think it was Biden', 'link': 'https://www.cnn.com/2025/10/13/politics/president-2020-biden-trump', 'source': 'CNN', 'source_logo': 'https://serpapi.com/searches/68ee4a73bb48adb2f8b82a75/images/6d54e8c6ada5c046e2fc8c62e49311a2ee155122f5c548c6e3b335f9575e0b49.png', 'date': '17 hours ago', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSAhk2OKtV34OaGwiC3b5Wmdq2lkKmGnZEVlMA-Tv4XZ2wte6_9xvIwyDA&usqp=CAI&s'}, {'title': 'The Trump Declaration for Enduring Peace and Prosperity', 'link': 'https://www.whitehouse.gov/presidential-actions/2025/10/the-trump-declaration-for-enduring-peace-and-prosperity/', 'source': 'The White House (.gov)', 'source_logo': 'https://serpapi.com/searches/68ee4a73bb48adb2f8b82a75/images/6d54e8c6ada5c04614febcdcfe5e178719304f3b47221ef8c16c0ee8e4d1cd8a.png', 'date': '16 hours ago'}]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results do not provide clear information on who will be the president in 2025. The articles seem to be speculative or from a fictional scenario. I will need to perform another search with a more direct approach.\n",
      "Action: Search\n",
      "Action Input: \"current president of the USA 2025\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Donald Trump is the 47th and current president since January 20, 2025. Contents. 1 History and development. 1.1 Origins; 1.2 1789–1933; 1.3 Imperial ...', 'President Donald J. Trump is returning to the White House to build upon his previous successes and use his mandate to reject the extremist policies.', 'The 47th and current president of the United States is Donald John Trump. He was sworn into office on January 20, 2025.', 'Official portrait, 2025. 45th & 47th President of the United States · Incumbent. Assumed office. January 20, 2025. Vice President · JD Vance. Preceded by, Joe ...', \"Trump is President of the United States and is a Republican. He has served since Jan. 20, 2025. Trump's current term ends on Jan. 20, 2029.\", 'Donald J. Trump. President of the United States · JD Vance. VICE PRESIDENT OF THE UNITED STATES · Melania Trump. First Lady OF THE UNITED STATES · The Cabinet. Of ...', '... President Donald J. Trump. @POTUS. 45th & 47th President of the United States. The Golden Age of America Begins Right Now. wh.gov Joined January 2025.', 'President Donald J. Trump 45th & 47th President of the United States. The Golden Age of America Begins Right Now.', 'President Donald J. Trump. 457712 likes · 97597 talking about this. 45th & 47th President of the United States. The Golden Age of America Begins Right Now.']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mBased on the search results, it appears that Donald Trump is the president of the USA in 2025. Next, I need to find out who his spouse is at this time.\n",
      "Action: Search\n",
      "Action Input: \"spouse of president trump 2025\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Melania Knauss Trump is a Slovenian and American former model who has served as the First Lady of the United States since 2025, as the wife of the 45th and 47th U.S. President Donald Trump. She previously held the role from 2017 to 2021.', 'Melania Trump type: First Lady of the United States.', 'Melania Trump entity_type: people.', 'Melania Trump kgmid: /m/04hfnv.', 'Melania Trump born: April 26, 1970 (age 55 years), Novo Mesto, Slovenia.', 'Melania Trump grandparents: Anton Ulčnik, Amalija Ulčnik.', 'Melania Trump books: Melania.', 'Melania Trump office: First Lady/Gentleman of the United States.', 'Melania Trump party: Republican Party.', 'Melania Trump previous_office: First Lady/Gentleman of the United States (2017–2021).', 'Melania Trump children: Barron Trump.', 'Melania Knauss Trump is a Slovenian and American former model who has served as the first lady of the United States since 2025, as the wife of the 45th and ...', 'First Lady Melania Trump is the wife of the 45th and 47th President of the United States, Donald J. Trump, and the mother to their son, Barron Trump.', 'The current first lady, since January 20, 2025, is Melania Trump, wife of President Donald Trump.', 'Melania Trump is a Slovenian American fashion model who served as first lady of the United States during the administrations (2017–21; 2025– ) ...', \"Melania Trump, 55, is Trump's third and current wife. She's a former model from Yugoslavia, and the second foreign-born first lady of the United ...\", 'Photo by First Lady Melania Trump on October 04, 2025. May be an image. I look forward to visiting our brave Sailors tomorrow. My memories ...', 'After her husband won the 2024 presidential election, Mrs. Trump returned to the White House in January 2025.', 'First Lady Melania Trump continues to transform the role of First Lady through results driven achievements: ✓ AI Innovation and Online Protection: TAKE IT ...', \"Celebrating 20 years with my beautiful wife and our incredible First Lady, Melania. You're an extraordinary wife and a wonderful mother.\", 'First Lady Melania Trump is the wife of the 45th President, Donald J. Trump and the mother to their son, Barron Trump. She is the second First Lady born ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results confirm that Melania Trump is the spouse of President Donald Trump in 2025.\n",
      "Final Answer: The president of the USA in 2025 is Donald Trump, and his spouse is Melania Trump.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me the name of the president of the USA in 2025, pause 10 seconds, and tell me the name of the spouse of the president',\n",
       " 'output': 'The president of the USA in 2025 is Donald Trump, and his spouse is Melania Trump.'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### your task ####\n",
    "# use a built-in tool in langchain to complete a task of your choice. \n",
    "# in the comment, please describe the task and the tool you used. \n",
    "# optional: try to use more than one tool in the same agent\n",
    "# from langchain.agents import get_all_tool_names\n",
    "# print(get_all_tool_names())\n",
    "\n",
    "tools = [load_tools([\"serpapi\", \"sleep\"], chat_model)[0]]\n",
    "agent_chain = initialize_agent(tools, chat_model, agent=\"zero-shot-react-description\", prompt_template=prompt, verbose=True)\n",
    "agent_chain.invoke(\"tell me the name of the president of the USA in 2025, pause 10 seconds, and tell me the name of the spouse of the president\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
